{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To answer the question \"What is LangChain?\", let\\'s break it down step by step:\\n\\n1. **Understanding the term**: LangChain is a relatively new project in the blockchain and Web3 space.\\n2. **Researching the project**: After some research, I found that LangChain is an open-source framework for building custom blockchain applications.\\n3. **Analyzing its purpose**: The primary goal of LangChain is to provide a set of tools and libraries that enable developers to build decentralized applications (dApps) on top of various blockchain networks, such as Ethereum.\\n4. **Exploring its features**: LangChain offers a range of features, including support for smart contracts, interoperability with other blockchain platforms, and the ability to create custom chain configurations.\\n5. **Considering its impact**: LangChain has the potential to simplify the development process for building complex blockchain applications, while also enabling greater flexibility and customization.\\n\\nIn summary, LangChain is an open-source framework that enables developers to build custom blockchain applications on top of various blockchain networks, with a focus on simplicity, flexibility, and interoperability.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MllamaForConditionalGeneration, AutoProcessor,MllamaProcessor\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-11B-Vision-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMllamaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m processor \u001b[38;5;241m=\u001b[39m MllamaProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath_to_Image\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Desktop/College/ollama_server/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3577\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3573\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3575\u001b[0m         )\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 3577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3578\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3579\u001b[0m         )\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image as PIL_Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor,MllamaProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "processor = MllamaProcessor.from_pretrained(model_id)\n",
    "\n",
    "with open(\"Path_to_Image\", \"rb\") as f:\n",
    "    raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "conversation = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True,tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \",processor.decode(output[0])[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An image is a two-dimensional representation of an object, person, place, or concept that is created using various forms of media, such as photography, painting, drawing, sculpture, digital art, or other visual mediums. Images can be static (not moving) or dynamic (moving).\n",
      "\n",
      "Images can convey meaning, evoke emotions, and communicate ideas through a combination of colors, shapes, textures, and other visual elements. They can be used to:\n",
      "\n",
      "1. Represent reality: Images can capture the likeness of real-world objects, people, or scenes.\n",
      "2. Express imagination: Images can represent abstract concepts, fantasies, or fictional worlds.\n",
      "3. Tell stories: Images can be used to convey narratives, emotions, and experiences.\n",
      "\n",
      "Types of images include:\n",
      "\n",
      "1. Photographs: Captured using a camera, often representing reality.\n",
      "2. Paintings: Created using various artistic mediums, such as oil paints or watercolors.\n",
      "3. Drawings: Rendered using pencils, pens, or other drawing tools.\n",
      "4. Digital art: Created using software and digital technologies, such as computer-aided design (CAD) or graphic design programs.\n",
      "5. Sculptures: Three-dimensional artworks created from various materials, like clay, metal, or stone.\n",
      "\n",
      "Images can be found in various contexts:\n",
      "\n",
      "1. Art: Images are used to create visual art, installations, or exhibitions.\n",
      "2. Advertising and marketing: Images are used to promote products, services, or ideas.\n",
      "3. Media: Images are featured in films, television shows, newspapers, magazines, and online publications.\n",
      "4. Education: Images are used as teaching tools, illustrating concepts, and enhancing learning experiences.\n",
      "5. Communication: Images are used to convey messages, express emotions, and build relationships.\n",
      "\n",
      "In summary, an image is a visual representation of something that can be perceived by the human eye, conveying meaning, emotion, or information through various forms of media.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'What is an image?',\n",
    "        # 'images': ['image.png']\n",
    "    }]\n",
    ")\n",
    "response = response['message']['content']\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
